# 人工神经网络ANN
模拟神经元:一个神经元有两种工作状态,兴奋和抑制状态
## M-P模型
![[Pasted image 20230430092051.png]]
多个输入信号通过线性组合输出,b叫做偏置
其中激活函数是一个阶跃函数![[Pasted image 20230430092226.png]]
这意味着不能采用基于梯度的优化方法,是一个简单的二类分类器.
MP模型的输入输出只能是0,1,而且权值阈值都是认为设置的.并且只能解决二分类问题,不能解决线性不可分问题.
## 感知机
![[Pasted image 20230430092417.png]]
由一个神经元构成的神经网络叫做感知机,也叫单层神经网络,有一组输入和一个输出
- 是一种简单的非线性神经网络,是人工神经网络的基础
- 通过有监督学习俩逐步加强分类能力
- 本质上与MP模型没有太大区别,而且功能也差不多

不同之处在于感知机自己学习权值和偏置,不需要人为输入
激活函数也不同
## 多层神经网络 MNN
![[Pasted image 20230430092803.png]]
其中计算层数的时候,只看隐含层和输出层的数量,比如上图为2
可以解决非线性可分问题
### 激活函数
又叫传递函数或者输出变换函数,目的是将线性转化为非线性,使得神经网络可以逼近任何非线性函数.
在mp模型中激活函数只是为了做一个阈值的作用,而在感知机中激活函数还起到的另一个重要作用就是使得模型可以逼近非线性模型
一般常用的激活函数如下
1. 阶跃函数![[Pasted image 20230506142918.png]]
2. ReLU函数,常用于卷积神经网络中![[Pasted image 20230506142933.png]]

# softmax
softmax函数也叫归一化指数函数,能够使得每个输出在0-1之间,并且所有元素和为1![[Pasted image 20230506143126.png]]
对输出向量进行归一化,凸显最大值,并且抑制远低于最大值的其他分量,sigmoid就是类别为2的softmax函数,softmax一个输入只能归为一类,但多个logistic回归可以实现多类别分类,比如苹果属于水果和食品
# 神经网络的连接方式
我们一般分为全连接神经网络(每个单元都和相邻层的所有单元链接)和部分链接神经网络(比如卷积神经网络BP)
根据层次之间的连接方式我们又可以分为前馈神经网络和反馈神经网络(最后一层的输出返回作为第一层神经元的输入).
![[Pasted image 20230506143520.png]]
多层前馈神经网络又叫多层前馈全连接网.反向误差传播算法是其中的杰出代表(BP算法),经常与梯度下降优化法结合使用.
# BP-NN学习算法
神经网络学习是指有效调整神经网络的连接权值或者结构,使得输入输出具有需要的特性,学习过程分为正向传播和反向传播两阶段.
1. 在正向传播时,输入信号逐层处理
2. 当输出层的结果与期望不同,就进入反向传播阶段,将误差信号沿着原来的链路返回,采用梯度下降法修改各层神经元的链接权值

其中学习算法的目标函数为
![[Pasted image 20230506144133.png]]
在反向传播的时候,我们需要让权重减去学习率(自定义的超参数)\*梯度来进行训练.
当然,BP算法也有自己的缺点,容易造成过拟合和欠拟合,我们可以采取早停法,随机失活(隐含层的权重随机归零)和数据增强(比如一张图片转几下)
因为它是一种局部搜索的优化方法,容易陷入局部极小值.
梯度消失和梯度爆炸,当梯度下降的梯度小于1时,在传播的时候因为累计相乘,导致梯度消失,而梯度大于1的时候,则梯度越来越大.为了解决这个问题,我们更换了激活函数为ReLU,因为RELU的梯度就是1,所以不存在这些问题()

# 卷积神经网络CNN
卷积神经网络就是在BP的基础上断开某些链接
他的4个关键思想为:局部连接,共享权值,池化(采样)和多层卷积
其中局部连接和权值共享是为了降低时间复杂度.
## CNN的基础结构
就是((卷积层+激活函数)\*N+池化层)\*M+全连接层(输出一定是全连接层)
![[Pasted image 20230506144851.png]]
有几个分类就有多少个全连接层输出.比如手写体识别,就有10个输出层.
## 卷积层
卷积层包含一组可以学习的滤波器,每个滤波器对输入数据进行卷积,计算得到一个新的结果![[Pasted image 20230506145107.png]]
如图,比如我们对灰度图进行卷积,右上角的矩阵就是卷积.每个滤波器就是一个神经元,滤波器覆盖的范围叫做感知野.深度就是RBG通道数,步长就是移动一次移动多少个距离.在灰度图上,过滤器就是卷积核.
卷积的作用就是对图像进行特征提取.
RBG图像上的单个过滤器:
一个图层一个卷积核,然后合体得到一个结果,滤波器就是各个图层上的卷积核的集合.
滤波器的深度就是前一层的输出数据的深度.合并的时候需要为每个图层设置权重,然后再加上一个偏置(唯一)得到结果
有多少个滤波器就输出多少层结果
![[Pasted image 20230506145802.png]]
多层卷积之后学习到的特征就越是全局化
其中滤波器个数就是超参数.
因为每次卷积之后图片的阶层就会减少,所以我们为了解决这个问题,就使用0填充图片的外延.
![[Pasted image 20230506150005.png]]
## 池化层
![[Pasted image 20230506150051.png]]
比如我们设置池化层的算法为范围内的最大值或者选择平均值
![[Pasted image 20230506150132.png]]




